use serde::{Deserialize, Serialize};


use alloc::vec::Vec;
use alloc::string::String;

#[derive(Serialize, Deserialize, Debug, PartialEq, Eq, Clone, Hash)]
pub enum TokenKinds {
    /// A sequence of characters
    Token(String),
    /// A string of characters that will be generated by the preprocessor
    Complex(String),
    Text,
    Whitespace,
    Control(ControlTokenKind),
}

#[derive(Debug, PartialEq, Eq, Clone, Hash, Serialize, Deserialize)]
pub enum ControlTokenKind {
    Eof,
    Eol,
}

#[derive(Serialize, Deserialize, Debug, Clone)]
pub struct Lexer {
    /// Possible token kinds
    token_kinds: Vec<String>,
    longest_token_size: usize,
}

#[derive(Debug, PartialEq, Eq, Clone, Hash, Serialize, Deserialize)]
pub struct Token {
    /// Index of the token in the text
    pub index: usize,
    /// Length of the token
    pub len: usize,
    /// Location for debugging
    pub location: TextLocation,
    /// Kind of token
    pub kind: TokenKinds,
}

#[derive(Debug, PartialEq, Eq, Clone, Hash, Serialize, Deserialize)]
pub struct TextLocation {
    pub line: usize,
    pub column: usize,
}

impl TextLocation {
    pub fn new(line: usize, column: usize) -> TextLocation {
        let line = line + 1;
        let column = column + 1;
        TextLocation { line, column }
    }
}

impl Lexer {
    pub fn new() -> Lexer {
        Lexer {
            token_kinds: Vec::new(),
            longest_token_size: 0,
        }
    }

    pub fn add_tokens(&mut self, tokens: &[String]) {
        self.token_kinds.reserve(tokens.len());
        for token in tokens {
            if token.len() > self.longest_token_size {
                self.longest_token_size = token.len();
            }
            self.add_token(token.clone());
        }
    }

    pub fn add_token(&mut self, token: String) {
        if token.len() > self.longest_token_size {
            self.longest_token_size = token.len();
        }
        // find the right place to insert the token
        //
        //  1. find the first token that is longer than the new token
        //  2. insert the new token before the first token that is longer
        //
        // This way the tokens are sorted by length
        // This is important for optimization of the lexer
        let index = self
            .token_kinds
            .iter()
            .position(|x| x.len() > token.len())
            .unwrap_or(self.token_kinds.len());
        self.token_kinds.insert(index, token);
    }

    /// Lexer for UTF-8 text
    pub fn lex_utf8(&self, text: &str) -> Vec<Token> {
        let chars = text.char_indices().collect::<Vec<(usize, char)>>();
        let len = chars.len();
        // the allocation is a guess, but it should be close enough
        let mut tokens = Vec::with_capacity(chars.len() / 4);
        let mut i = 0;
        let mut line = 0;
        let mut column = 0;
        'chars: while i < len {
            // Take new line into account
            if chars[i].1 == '\n' {
                line += 1;
                column = 0;
                tokens.push(Token {
                    index: chars[i].0,
                    len: 1,
                    location: TextLocation::new(line, column),
                    kind: TokenKinds::Control(ControlTokenKind::Eol),
                });
                i += 1;
                continue;
            }

            // Match token kinds
            for token_kind in &self.token_kinds {
                let tok_len = token_kind.len();
                if i + tok_len >= len {
                    // All the remaining tokens are longer than the remaining text
                    //
                    // This is a performance optimization
                    break;
                }
                let token = &text[chars[i].0..chars[i + tok_len].0];
                if token == *token_kind {
                    tokens.push(Token {
                        index: chars[i].0,
                        len: tok_len,
                        location: TextLocation::new(line, column),
                        kind: TokenKinds::Token(token_kind.clone()),
                    });
                    i += tok_len;
                    column += tok_len;
                    continue 'chars;
                }
            }

            // Match whitespace
            if chars[i].1.is_whitespace() {
                tokens.push(Token {
                    index: chars[i].0,
                    len: 1,
                    location: TextLocation::new(line, column),
                    kind: TokenKinds::Whitespace,
                });
                i += 1;
                column += 1;
                continue;
            }

            // Match text until next whitespace/token/eof
            let mut j = 0;
            'word: while i + j < len {
                if chars[i + j].1.is_whitespace() {
                    break;
                }
                j += chars[i + j].1.len_utf8();
                for token_kind in &self.token_kinds {
                    let start = i + j;
                    let tok_len = token_kind.chars().count();
                    let end = if i + j + tok_len < len {
                        i + j + tok_len
                    } else {
                        break 'word;
                    };
                    let token = &text[chars[start].0..chars[end].0];
                    if token == *token_kind {
                        break 'word;
                    }
                }
            }
            tokens.push(Token {
                index: chars[i].0,
                len: j,
                location: TextLocation::new(line, column),
                kind: TokenKinds::Text,
            });
            column += j;
            i += j;
        }
        tokens.push(Token {
            index: i,
            len: 0,
            location: TextLocation::new(line, column),
            kind: TokenKinds::Control(ControlTokenKind::Eof),
        });
        tokens
    }

    /// Lexer for ascii-only text
    pub fn lex_ascii(&mut self, text: &str) -> Vec<Token> {
        let chars = text.as_bytes();
        // the allocation is a guess, but it should be close enough
        let mut tokens = Vec::with_capacity(chars.len() / 4);
        let mut i = 0;
        let mut line = 0;
        let mut column = 0;
        let len = chars.len();
        'chars: while i < len {
            // Take new line into account
            if chars[i] == '\n' as u8 {
                line += 1;
                column = 0;
                i += 1;
                tokens.push(Token {
                    index: i,
                    len: 1,
                    location: TextLocation::new(line, column),
                    kind: TokenKinds::Control(ControlTokenKind::Eol),
                });
                continue;
            }

            for token_kind in &self.token_kinds {
                let tok_len = token_kind.len();
                if i + tok_len > len {
                    // All the remaining tokens are longer than the remaining text
                    //
                    // This is a performance optimization
                    break;
                }
                let token = &chars[i..i + tok_len];
                if token == token_kind.as_bytes() {
                    tokens.push(Token {
                        index: i,
                        len: tok_len,
                        location: TextLocation::new(line, column),
                        kind: TokenKinds::Token(token_kind.clone()),
                    });
                    i += tok_len;
                    column += tok_len;
                    continue 'chars;
                }
            }

            // Match whitespace
            if (chars[i] as char).is_whitespace() {
                tokens.push(Token {
                    index: i,
                    len: 1,
                    location: TextLocation::new(line, column),
                    kind: TokenKinds::Whitespace,
                });
                i += 1;
                column += 1;
                continue;
            }

            // Match text until next whitespace/token/eof
            let mut j = 0;
            'word: while i + j < len {
                if (chars[i + j] as char).is_whitespace() {
                    break;
                }
                j += 1;
                for token_kind in &self.token_kinds {
                    let start = i + j;
                    let tok_len = token_kind.len();
                    let end = if i + j + tok_len <= len {
                        i + j + tok_len
                    } else {
                        break 'word;
                    };
                    let token = &text[start..end];
                    if token == *token_kind {
                        break 'word;
                    }
                }
            }
            tokens.push(Token {
                index: i,
                len: j,
                location: TextLocation::new(line, column),
                kind: TokenKinds::Text,
            });
            column += j;
            i += j;
        }

        tokens.push(Token {
            index: i,
            len: 0,
            location: TextLocation::new(line, column),
            kind: TokenKinds::Control(ControlTokenKind::Eof),
        });
        tokens
    }

    /// Takes a slice of tokens and returns a string of the text
    pub fn stringify_slice<'a>(&self, tokens: &[Token], text: &'a str) -> &'a str {
        let start = match tokens.first() {
            Some(token) => token.index,
            None => return "",
        };
        let end = match tokens.last() {
            Some(token) => token.index + token.len,
            None => return "",
        };
        if start >= end {
            return "";
        }
        if end > text.len() {
            return "";
        }
        &text[start..end]
    }

    pub fn stringify<'a>(&self, token: &Token, text: &'a str) -> &'a str {
        &text[token.index..token.index + token.len]
    }
}
