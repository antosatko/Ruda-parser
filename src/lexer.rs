#[derive(Debug, PartialEq, Eq, Clone, Hash)]
pub enum TokenKinds {
    /// A sequence of characters
    Token(String),
    /// A string of characters that will be generated by the preprocessor
    Complex(String),
    Text,
    Whitespace,
    Control(ControlTokenKind),
}

#[derive(Debug, PartialEq, Eq, Clone, Hash)]
pub enum ControlTokenKind {
    Eof,
    Eol,
}

pub struct Lexer<'a> {
    /// Reference to the text being lexed
    pub(crate) text: &'a str,
    /// Possible token kinds
    token_kinds: Vec<String>,
    /// Tokens that have been lexed
    pub tokens: Vec<Token>,
}

#[derive(Debug, PartialEq, Eq, Clone, Hash)]
pub struct Token {
    /// Index of the token in the text
    pub index: usize,
    /// Length of the token
    pub len: usize,
    /// Location for debugging
    pub location: TextLocation,
    /// Kind of token
    pub kind: TokenKinds,
}

#[derive(Debug, PartialEq, Eq, Clone, Hash)]
pub struct TextLocation {
    pub line: usize,
    pub column: usize,
}

impl TextLocation {
    pub fn new(line: usize, column: usize) -> TextLocation {
        let line = line + 1;
        let column = column + 1;
        TextLocation { line, column }
    }
}

impl<'a> Lexer<'a> {
    pub fn new(text: &'a str) -> Lexer<'a> {
        Lexer {
            text,
            token_kinds: Vec::new(),
            tokens: Vec::new(),
        }
    }

    pub fn add_tokens(&mut self, tokens: &[String]) {
        self.token_kinds.reserve(tokens.len());
        for token in tokens {
            self.token_kinds.push(token.clone());
        }
    }

    pub fn add_token(&mut self, token: String) {
        self.token_kinds.push(token);
    }

    pub fn lex(&mut self) -> Vec<Token> {
        let chars = self.text.char_indices().collect::<Vec<(usize, char)>>();
        let mut tokens = Vec::new();
        let mut i = 0;
        let mut line = 0;
        let mut column = 0;
        'chars: while i < chars.len() {
            // Take new line into account
            if chars[i].1 == '\n' {
                line += 1;
                column = 0;
                tokens.push(Token {
                    index: chars[i].0,
                    len: 1,
                    location: TextLocation::new(line, column),
                    kind: TokenKinds::Control(ControlTokenKind::Eol),
                });
                i += 1;
                continue;
            }

            // Match token kinds
            for token_kind in &self.token_kinds {
                let mut token;
                let mut j: usize = 0;
                while i + j < chars.len() {
                    //token = &self.text[i..i + j + 1];
                    println!("{:?}", chars);
                    let start = chars[i].0;
                    let end = if i + j + 1 < chars.len() {
                        chars[i + j + 1].0
                    } else {
                        chars[i + j].0 + chars[i + j].1.len_utf8()
                    };
                    token = &self.text[start..end];
                    if token == *token_kind {
                        tokens.push(Token {
                            index: chars[i].0,
                            len: j + 1,
                            location: TextLocation::new(line, column),
                            kind: TokenKinds::Token(token.to_string()),
                        });
                        i += j + 1;
                        column += j + 1;
                        continue 'chars;
                    }
                    j += 1;
                }
            }

            // Match whitespace
            if chars[i].1.is_whitespace() {
                tokens.push(Token {
                    index: chars[i].0,
                    len: 1,
                    location: TextLocation::new(line, column),
                    kind: TokenKinds::Whitespace,
                });
                i += 1;
                column += 1;
                continue;
            }

            // Match text until next whitespace/token/eof
            let mut j = 0;
            'word: while i + j < chars.len() {
                if chars[i + j].1.is_whitespace() {
                    break;
                }
                for token_kind in &self.token_kinds {
                    // FIXME: A loooooot of unnecessary allocations
                    if chars[i + j..]
                        .iter()
                        .map(|(_, char)| *char)
                        .collect::<Vec<char>>()
                        .starts_with(&token_kind.chars().collect::<Vec<char>>())
                    {
                        break 'word;
                    }
                }
                j += 1;
            }
            tokens.push(Token {
                index: chars[i].0,
                len: j,
                location: TextLocation::new(line, column),
                kind: TokenKinds::Text,
            });
            column += j;
            i += j;
        }
        tokens.push(Token {
            index: i,
            len: 0,
            location: TextLocation::new(line, column),
            kind: TokenKinds::Control(ControlTokenKind::Eof),
        });
        tokens
    }

    /// Takes a slice of tokens and returns a string of the text
    pub fn stringify_slice(&self, tokens: &[Token]) -> String {
        let start = match tokens.first() {
            Some(token) => token.index,
            None => return String::new(),
        };
        let end = match tokens.last() {
            Some(token) => token.index + token.len,
            None => return String::new(),
        };
        if start >= end {
            return String::new();
        }
        if end > self.text.len() {
            return String::new();
        }
        self.text[start..end].to_string()
    }

    pub fn stringify(&self, token: &Token) -> String {
        self.text[token.index..token.index + token.len].to_string()
    }
}
